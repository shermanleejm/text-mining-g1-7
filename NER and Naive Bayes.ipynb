{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Named Entity Extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import json\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "import nltk\n",
    "import math\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## covid19_articles_20200914.csv\n",
    "# sample = pd.read_csv(\"sample.csv\", index_col=[0])\n",
    "sample = pd.read_csv(\"covid19_articles_20200914.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named entities are NP that are an OPTIONAL DT followed by any num of JJ and then a NN\n",
    "## https://www.nltk.org/book/ch07.html\n",
    "## Regex is \n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thursday\nJanuary\nThomas\nHughes\nIndustrial\nIndustrial\nSector\nS&P\nIndustrial\nSector\nETF\nXLI\nYes\nEnergy\nSector\nEPS\nEnergy\nSector\nXLE\nEPS\nIndustrial\nSector\nS&P\nEnergy\nSector\nIndustrial\nEnergy\nDon\nBet\nBoeing\nBoeing\nBA\nIndustrial\nSector\nSPDR\nHoneywell\nHON\nMax\nBoeing\nCEO\nDennis\nMuilenberg\nMuilenberg\nBoeing\nMax\nNativeDisplayAdID\nBuffett\nWhich\nWall\nStreet\nLegend\nBuffett\nBuffett\nAmerica\nXLI\nCaterpillar\nCAT\nDeere\nCompany\nDE\nCummins\nInc\nCMI\nPhase\nOne\nDeal\nCummins\nInc\nMove\nCummins\nInc\nCummins\nAugust\nPhase\nOne\nDeal\nCummins\nCummins\nXLI\nDeere\nCompany\nChina\nKey\nGrowth\nDeere\nCompany\nPhase\nOne\nDeal\nPhase\nOne\nDeal\nChina\nChina\nDeere\nCompany\nEPS\nEPS\nCummins\nDeere\nCompany\nCaterpillar\nA\nDividend\nAristocrat\nCapital\nGains\nShares\nCaterpillar\nCaterpillar\nCEO\nFlag\nPattern\nCaterpillar\nDividend\nAristocrat\nStocks\nWill\nBenefit\nFederal\nReserve\nFederal\nReserve\nFederal\nReserve\nChina\nFed\nJuly\nFed\nOctober\nJanuary\nFebruary\nFebruary\nFed\nU\nS\nCovid\nView\nFederal\nReserve\nComplete\n"
     ]
    }
   ],
   "source": [
    "## this is what is taught in class\n",
    "for line in sample[:1].iterrows():\n",
    "    ## converts the content into lower and split thereby tokenising\n",
    "    content = re.sub(\"[^0-9a-zA-Z\\&]+\", \" \",line[1]['content']).split(\" \")\n",
    "    content = [ _ for _ in content if _ not in [\"\", \"xli\", \"x\", \"xle\", \"xar\", \"xly\"] ]\n",
    "\n",
    "    ## tags the content by parts of speech\n",
    "    content_pos = pos_tag(content)\n",
    "\n",
    "    ## get named entities using our pattern\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    chunk_sentence = cp.parse(content_pos)\n",
    "    iob_tagged = tree2conlltags(chunk_sentence)\n",
    "\n",
    "    for _ in iob_tagged:\n",
    "        if _[1] == \"NNP\":\n",
    "            print (_[0])\n",
    "\n",
    "## results arent that good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Industrial\nIndustrial\nETF\nEnergy\nEPS\nEnergy\nXLE\nEPS\nIndustrial\nEnergy\nIndustrial\nEnergy\nIndustrial\nHoneywell\nXLI\nBuffett\nWhich\nBuffett\nBuffett\nAmerica\nXLI\nCummins\nCummins\nCummins\nXLI\nChina\nChina\nCompany\nEPS\nCompany\nCEO\nChina\nFed\nFed\n"
     ]
    }
   ],
   "source": [
    "## use nltk named entity function\n",
    "for line in sample[:1].iterrows():\n",
    "    ## converts the content into lower and split thereby tokenising\n",
    "    content = re.sub(\"[^0-9a-zA-Z\\&]+\", \" \",line[1]['content']).split(\" \")\n",
    "    content = [ _ for _ in content if _ not in [\"\", \"xli\", \"x\", \"xle\", \"xar\", \"xly\"] ]\n",
    "\n",
    "    ## create named entity tree\n",
    "    ne_tree = nltk.ne_chunk(pos_tag(content), binary=True)\n",
    "    for entity in ne_tree:\n",
    "        if len(entity) == 1:\n",
    "            ## use entity[0] to get the token\n",
    "            print(entity[0][0])\n",
    "\n",
    "## slightly better but still pretty bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use spaCy\n",
    "import spacy \n",
    "from spacy import displacy \n",
    "## python -m spacy download en_core_web_sm \n",
    "## theres md and lg \n",
    "## en_core_web_sm is a CNN for token vectors, POS tags\n",
    "## pretrained statistical models for English\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1058836 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-70a6fff7ecee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z\\&]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSIGNIGICANT_ENTITY_LABELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    439\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1058836 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "## only concerned with the named entities that involve human names\n",
    "SIGNIGICANT_ENTITY_LABELS = [385, 381, 380]\n",
    "\n",
    "ne_count = {}\n",
    "for line in sample.iterrows(): \n",
    "    content = re.sub(\"[^a-zA-Z\\&]+\", \" \",line[1]['content'])\n",
    "    content = nlp(content)\n",
    "    for ent in content.ents:\n",
    "        if ent.label in SIGNIGICANT_ENTITY_LABELS:\n",
    "            ent_text = ent.text.lower()\n",
    "            if ne_count.get(ent_text, False) == False:\n",
    "                ne_count[ent_text] = 0\n",
    "            ne_count[ent_text] += 1\n",
    "\n",
    "with open(\"named_entities.json\", \"w+\") as fp:\n",
    "    json.dump(ne_count, fp, indent=2)\n"
   ]
  },
  {
   "source": [
    "# Naive Bayes Classification (manual)\n",
    "\n",
    "### P(news article|Topic1) == P(word1|Topic1) * P(word2|Topic1) * ... \n",
    "\n",
    "### P(news article|Topic2) == P(word1|Topic2) * P(word2|Topic2) * ... \n",
    "\n",
    ". \n",
    ".\n",
    ".\n",
    "\n",
    "### Find the probability of a news article to each topic to find which topic it is most likely"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordToTopic(df):\n",
    "    word_to_topic = {}\n",
    "    num_of_words = {}\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        topic = row[1][\"topic_area\"]\n",
    "        content = re.sub(\"[^a-zA-Z\\&]+\", \" \", row[1]['content']).lower().split(\" \")\n",
    "        content = [ _ for _ in content if _ not in stopwords ]\n",
    "\n",
    "        if word_to_topic.get(topic, None) == None:\n",
    "            word_to_topic[topic] = {}\n",
    "\n",
    "        if num_of_words.get(topic, None) == None:\n",
    "            num_of_words[topic] = 0\n",
    "\n",
    "        for word in content:\n",
    "            if word_to_topic[topic].get(word, None) == None:\n",
    "                word_to_topic[topic][word] = 0\n",
    "            word_to_topic[topic][word] += 1\n",
    "            num_of_words[topic] += 1\n",
    "    \n",
    "    # for topic, value in word_to_topic.items():\n",
    "    #     for word, num_occurence in value.items():\n",
    "    #         value[word] = num_occurence / num_of_words[topic] * 100\n",
    "    \n",
    "    return (word_to_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainTest = pd.read_csv(\"sample.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 29047 entries, 0 to 29046\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   title         29047 non-null  object\n 1   url           29047 non-null  object\n 2   crawled_time  29047 non-null  object\n 3   date          29047 non-null  object\n 4   domain        29047 non-null  object\n 5   author        19635 non-null  object\n 6   content       29047 non-null  object\n 7   topic_area    29047 non-null  object\ndtypes: object(8)\nmemory usage: 2.0+ MB\n"
    }
   ],
   "source": [
    "trainTest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trainTest[: int(len(trainTest) * 0.9) ]\n",
    "test = trainTest[ int(len(trainTest) * 0.9): ]\n",
    "\n",
    "## call the function to generate it again or used the pre generated JSON file\n",
    "# word_to_topic = createWordToTopic(train)\n",
    "# with open(\"word_to_topic_raw_count.json\", \"w+\") as fp:\n",
    "#     json.dump(word_to_topic, fp, indent=2)\n",
    "word_to_topic = json.load(open(\"word_to_topic_raw_count.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dict_keys(['business', 'general', 'science', 'finance', 'tech', 'healthcare', 'automotive', 'environment', 'ai'])\n"
    }
   ],
   "source": [
    "print (word_to_topic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "123189\n"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for key, val in word_to_topic.items():\n",
    "    for word in val:\n",
    "        vocab.add(word)\n",
    "\n",
    "print (len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "manual_result = []\n",
    "for row in test[:2].iterrows():\n",
    "    actual_topic = row[1][\"topic_area\"]\n",
    "    \n",
    "    tokens = re.sub(\"[^a-zA-Z\\&]+\", \" \",row[1][\"content\"]).lower().split(\" \")\n",
    "    tokens = [ x for x in tokens if x != \"\" and x not in stopwords ]\n",
    "\n",
    "    highest_score = 0\n",
    "    guess_topic = \"\"\n",
    "    for topic, array in word_to_topic.items():\n",
    "        score = 0\n",
    "        for word in tokens:\n",
    "            score += math.log( (array.get(word, 0) + 1) / len(train[train[\"topic_area\"].str.contains(topic)]) + len(vocab) )\n",
    "        \n",
    "        if score > highest_score:\n",
    "            highest_score = score\n",
    "            guess_topic = topic\n",
    "    \n",
    "    if topic == actual_topic:\n",
    "        num_correct += 1\n",
    "    manual_result.append( {\"actual\": actual_topic, \"guess\": guess_topic} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "actual: science\nguess: environment\n\nactual: science\nguess: environment\n\n"
    }
   ],
   "source": [
    "for _ in manual_result:\n",
    "    for k, v in _.items():\n",
    "        print (f\"{k}: {v}\")\n",
    "    print ()"
   ]
  },
  {
   "source": [
    "# Naive Bayes Classifier (library)\n",
    "\n",
    "https://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainTest = pd.read_csv(\"covid19_articles_20200914.csv\")\n",
    "\n",
    "## convert label to a number\n",
    "topic_areas = list(trainTest[\"topic_area\"].unique())\n",
    "topic_to_number = { topic_areas[idx]:idx for idx in range(len(topic_areas)) }\n",
    "trainTest[\"label\"] = trainTest[\"topic_area\"].apply(lambda x: topic_to_number[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    210272\n",
       "1     53089\n",
       "3     19892\n",
       "4      4256\n",
       "2      1953\n",
       "5       400\n",
       "7       335\n",
       "6       267\n",
       "8        15\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "trainTest[\"label\"].value_counts()\n",
    "## this might be bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainTest[\"content\"], trainTest[\"label\"], random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(strip_accents=\"ascii\", token_pattern=u\"(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b\", lowercase=True, stop_words=\"english\")\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_cv, y_train)\n",
    "predictions = naive_bayes.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score:  0.5757780225833103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}