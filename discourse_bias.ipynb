{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit744d58b894bb4db782f8537331854fe8",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from networkx.algorithms.centrality import betweenness_centrality\n",
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from datetime import datetime\n",
    "\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "## for more comprehensive use the large one\n",
    "## python -m spacy download en_core_web_lg\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "SAVED_STATE_FOLDER = \"/Users/shrmnl/Github/text-mining-g1-7/saved_states/\"\n",
    "SAVED_FIGURES = \"/Users/shrmnl/Github/text-mining-g1-7/plt_figures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(json.load(open(f\"{SAVED_STATE_FOLDER}/df_processed_lda.json\")), orient=\"records\")\n",
    "df = pd.read_csv(\"./sample.csv\")"
   ]
  },
  {
   "source": [
    "## Assumptions for discourse bias analysis\n",
    "1. Each word is connected to another if they are found in the same sentence\n",
    "2. The weight of the edge is the number of times each word is connected\n",
    "3. Stop words are removed\n",
    "4. Most influential words are those with the largest betweeness centrality\n",
    "5. A graph with modularity measure of less than 0.4 would be considered bias (definition of modularity can be found here https://arxiv.org/abs/0803.0476)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_num  = 0\n",
    "\n",
    "def get_bias(text, index, visualise=False, filename=\"graph.png\", folder=False):\n",
    "    G = nx.Graph()\n",
    "    ## convert lowercase\n",
    "    # first_article = df.head().loc[row_num][\"content\"].lower()\n",
    "    first_article = text.lower()\n",
    "    ## remove html tags from improper scraping\n",
    "    first_article = re.sub(\"\\<(.)+\\>\", \" \", first_article)\n",
    "    ## replace non alpha-numeric + . + & + \\s with \\s\n",
    "    ## kept the apostraphe â€™ chr(8217)\n",
    "    special_apostraphe = chr(8217) \n",
    "    first_article = re.sub(f\"[^0-9a-zA-Z\\.\\&\\s{special_apostraphe}]+\", \" \", first_article)\n",
    "    ## remove random new lines\n",
    "    first_article = re.sub(f\"\\n\", \" \", first_article)\n",
    "\n",
    "    ## loop through the sentences\n",
    "    for sentence in first_article.split(\". \"):\n",
    "        ## ignore empty strings\n",
    "        if sentence.strip() != \"\":\n",
    "            cleaned_sentence = [stemmer.stem(word) for word in sentence.split(\" \") if word not in spacy_stopwords and word.strip() != \"\"]\n",
    "            for i in range(len(cleaned_sentence) - 1):\n",
    "                word = cleaned_sentence[i]\n",
    "                for other_word in cleaned_sentence[i + 1:]:\n",
    "                    if G.get_edge_data(word, other_word) == None:\n",
    "                        G.add_edge(word, other_word, weight=0)\n",
    "                    old_weight = G.get_edge_data(word, other_word)[\"weight\"]\n",
    "                    new_weight = old_weight + 1\n",
    "                    G.add_edge(word, other_word, weight=new_weight)\n",
    "\n",
    "    ## remove edges that have very small weights\n",
    "    # to_remove = [(a,b) for a, b in G.edges if G[a][b][\"weight\"] == 1]\n",
    "    # G.remove_edges_from(to_remove)\n",
    "    # G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "    ## remove nodes that are in isolated pairs and triplets\n",
    "    for island in list(nx.connected_components(G)):\n",
    "        if len(island) < 11:\n",
    "            for node in island:\n",
    "                G.remove_node(node)\n",
    "\n",
    "    ## get list of betweenness_centrality scores to find most influential words\n",
    "    top_words_with_scores = {k: v for k, v in sorted(betweenness_centrality(G).items(), key=lambda item: item[1], reverse=True)[:5] if v > 0} \n",
    "    # print (f\"The top 5 key words are \", end=\"\")\n",
    "    # for word in top_5_words_with_scores.keys():\n",
    "    #     print (word, end=\", \")\n",
    "    # print ()\n",
    "\n",
    "    ## determining modularity\n",
    "    modularity_score = nx_comm.modularity(G, nx_comm.label_propagation_communities(G))\n",
    "    # print(f\"The modularity score is {modularity_score}\")\n",
    "\n",
    "\n",
    "    if folder:\n",
    "        filename = folder + str(index) + \" - \" + str(modularity_score) + \".png\"\n",
    "\n",
    "    ## for visualisation\n",
    "    plt.figure(figsize=(100,100))\n",
    "    pos = nx.spring_layout(G)\n",
    "    pos_higher = {}\n",
    "    y_off = 10 ## offset value\n",
    "    for k, v in pos.items():\n",
    "        pos_higher[k] = (v[0], v[1]+y_off)\n",
    "    nx.draw(G, pos_higher, with_labels=True, node_size=60)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if folder:\n",
    "        plt.savefig(f\"{SAVED_FIGURES}{filename}\")\n",
    "        \n",
    "    if visualise:\n",
    "        plt.show()\n",
    "\n",
    "    return modularity_score, list(top_words_with_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "get_bias() missing 1 required positional argument: 'index'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-cd2da09334c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## generate modularity score and top words for all the news agencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodularity_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodularity_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_bias() missing 1 required positional argument: 'index'"
     ]
    }
   ],
   "source": [
    "## generate modularity score and top words for all the news agencies\n",
    "for i, row in df.iterrows():\n",
    "    modularity_score, top_words = get_bias(text=row[\"content\"], index=i)\n",
    "    df.loc[i, \"bias\"] = modularity_score\n",
    "    try:\n",
    "        top_words_string = top_words[0]\n",
    "        for word in top_words[1:]:\n",
    "            top_words_string += \",\" + word\n",
    "    except:\n",
    "        top_words_string = \"\"\n",
    "    df.loc[i, \"top_words\"] = top_words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'bias'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bias'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a0b25c12c66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcnn_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bias'"
     ]
    }
   ],
   "source": [
    "## comparison between bias and unbiased\n",
    "## List of prominent news agencies \n",
    "## ['cnn', 'theguardian', 'nature', 'scientificamerican', 'nytimes','cnbc', 'bbc','scmp', 'reuters', 'finance.yahoo','theverge','independent', 'newyorker','japantimes','hbr']\n",
    "\n",
    "NEWS_AGENCY = \"cnbc\"\n",
    "\n",
    "cnn_df = df[df[\"domain\"] == NEWS_AGENCY]\n",
    "\n",
    "for i in range(0, 100, 10):\n",
    "    i = i / 100\n",
    "    data_range = cnn_df[(cnn_df[\"bias\"] > i) & (cnn_df[\"bias\"] < (i + 0.1))]\n",
    "    for i, row in data_range.sample(5).iterrows():\n",
    "        try:\n",
    "            modularity_score, top_words = get_bias(text=row[\"content\"], folder=f\"{NEWS_AGENCY}_figures/\", index=i)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}