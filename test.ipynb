{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600666787421",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Named Entity Extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import json\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "import nltk\n",
    "import math\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"sample.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named entities are NP that are an OPTIONAL DT followed by any num of JJ and then a NN\n",
    "## https://www.nltk.org/book/ch07.html\n",
    "## Regex is \n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Thursday\nJanuary\nThomas\nHughes\nIndustrial\nIndustrial\nSector\nS&P\nIndustrial\nSector\nETF\nXLI\nYes\nEnergy\nSector\nEPS\nEnergy\nSector\nXLE\nEPS\nIndustrial\nSector\nS&P\nEnergy\nSector\nIndustrial\nEnergy\nDon\nBet\nBoeing\nBoeing\nBA\nIndustrial\nSector\nSPDR\nHoneywell\nHON\nMax\nBoeing\nCEO\nDennis\nMuilenberg\nMuilenberg\nBoeing\nMax\nNativeDisplayAdID\nBuffett\nWhich\nWall\nStreet\nLegend\nBuffett\nBuffett\nAmerica\nXLI\nCaterpillar\nCAT\nDeere\nCompany\nDE\nCummins\nInc\nCMI\nPhase\nOne\nDeal\nCummins\nInc\nMove\nCummins\nInc\nCummins\nAugust\nPhase\nOne\nDeal\nCummins\nCummins\nXLI\nDeere\nCompany\nChina\nKey\nGrowth\nDeere\nCompany\nPhase\nOne\nDeal\nPhase\nOne\nDeal\nChina\nChina\nDeere\nCompany\nEPS\nEPS\nCummins\nDeere\nCompany\nCaterpillar\nA\nDividend\nAristocrat\nCapital\nGains\nShares\nCaterpillar\nCaterpillar\nCEO\nFlag\nPattern\nCaterpillar\nDividend\nAristocrat\nStocks\nWill\nBenefit\nFederal\nReserve\nFederal\nReserve\nFederal\nReserve\nChina\nFed\nJuly\nFed\nOctober\nJanuary\nFebruary\nFebruary\nFed\nU\nS\nCovid\nView\nFederal\nReserve\nComplete\n"
    }
   ],
   "source": [
    "## this is what is taught in class\n",
    "for line in sample[:1].iterrows():\n",
    "    ## converts the content into lower and split thereby tokenising\n",
    "    content = re.sub(\"[^0-9a-zA-Z\\&]+\", \" \",line[1]['content']).split(\" \")\n",
    "    content = [ _ for _ in content if _ not in [\"\", \"xli\", \"x\", \"xle\", \"xar\", \"xly\"] ]\n",
    "\n",
    "    ## tags the content by parts of speech\n",
    "    content_pos = pos_tag(content)\n",
    "\n",
    "    ## get named entities using our pattern\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    chunk_sentence = cp.parse(content_pos)\n",
    "    iob_tagged = tree2conlltags(chunk_sentence)\n",
    "\n",
    "    for _ in iob_tagged:\n",
    "        if _[1] == \"NNP\":\n",
    "            print (_[0])\n",
    "\n",
    "## results arent that good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Industrial\nIndustrial\nETF\nEnergy\nEPS\nEnergy\nXLE\nEPS\nIndustrial\nEnergy\nIndustrial\nEnergy\nIndustrial\nHoneywell\nXLI\nBuffett\nWhich\nBuffett\nBuffett\nAmerica\nXLI\nCummins\nCummins\nCummins\nXLI\nChina\nChina\nCompany\nEPS\nCompany\nCEO\nChina\nFed\nFed\n"
    }
   ],
   "source": [
    "## use nltk named entity function\n",
    "for line in sample[:1].iterrows():\n",
    "    ## converts the content into lower and split thereby tokenising\n",
    "    content = re.sub(\"[^0-9a-zA-Z\\&]+\", \" \",line[1]['content']).split(\" \")\n",
    "    content = [ _ for _ in content if _ not in [\"\", \"xli\", \"x\", \"xle\", \"xar\", \"xly\"] ]\n",
    "\n",
    "    ## create named entity tree\n",
    "    ne_tree = nltk.ne_chunk(pos_tag(content), binary=True)\n",
    "    for entity in ne_tree:\n",
    "        if len(entity) == 1:\n",
    "            ## use entity[0] to get the token\n",
    "            print(entity[0][0])\n",
    "\n",
    "## slightly better but still pretty bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use spaCy\n",
    "import spacy \n",
    "from spacy import displacy \n",
    "## python -m spacy download en_core_web_sm \n",
    "## theres md and lg \n",
    "## en_core_web_sm is a CNN for token vectors, POS tags\n",
    "## pretrained statistical models for English\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('Thursday January', 391), ('Thomas Hughes', 380), ('the Energy Sector', 383), ('EPS', 383), ('XLE', 383), ('EPS', 383), ('this year', 391), ('The Industrial Sector', 388), ('next year', 391), ('S&P', 383), ('only this year', 391), ('two year', 391), ('Energy', 383), ('Don t', 380), ('Boeing Boeing BA', 383), ('the Industrial Sector SPDR', 385), ('Honeywell', 380), ('HON', 383), ('XLI', 383), ('today', 391), ('Max', 380), ('Boeing', 383), ('Dennis Muilenberg Muilenberg', 380), ('Boeing', 383), ('Max', 380), ('rel nofollow', 380), ('href https www', 380), ('aspx NativeDisplayAdID &ImpressionID &UserID &Placement', 383), ('this Wall Street Legend s', 387), ('Buffett', 380), ('Buffett', 380), ('America', 384), ('XLI', 383), ('three', 397), ('today', 391), ('Caterpillar CAT Deere & Company DE', 383), ('Cummins Inc CMI', 383), ('One', 397), ('Cummins', 383), ('last August', 391), ('One', 397), ('Cummins', 383), ('Cummins', 383), ('today', 391), ('XLI', 383), ('Deere & Company China', 383), ('The Key To Growth Deere & Company', 383), ('One', 397), ('One', 397), ('China', 384), ('China', 384), ('Deere & Company', 383), ('one', 397), ('EPS', 383), ('Like Cummins Deere & Company', 383), ('today', 391), ('next year', 391), ('s', 386), ('Caterpillar A Dividend Aristocrat For Capital Gains', 383), ('Caterpillar', 383), ('the second half', 391), ('the last four months', 391), ('this year', 391), ('Caterpillar', 383), ('Caterpillar', 383), ('today', 391), ('the Federal Reserve', 383), ('the Federal Reserve', 383), ('China', 384), ('Fed', 383), ('July', 391), ('Fed', 383), ('October', 391), ('January', 391), ('early February', 391), ('mid February', 391), ('Fed', 383), ('one', 397), ('U S', 383), ('six', 397), ('Covid', 381), ('a Dovish Federal Reserve', 383), ('daily', 391)]\n"
    }
   ],
   "source": [
    "for line in sample[:1].iterrows(): \n",
    "    content = re.sub(\"[^a-zA-Z\\&]+\", \" \",line[1]['content'])\n",
    "    content = nlp(content)\n",
    "    print ([(_.text, _.label) for _ in content.ents])"
   ]
  },
  {
   "source": [
    "# Naive Bayes Classification (manual)\n",
    "\n",
    "### P(news article|Topic1) == P(word1|Topic1) * P(word2|Topic1) * ... \n",
    "\n",
    "### P(news article|Topic2) == P(word1|Topic2) * P(word2|Topic2) * ... \n",
    "\n",
    ". \n",
    ".\n",
    ".\n",
    "\n",
    "### Find the probability of a news article to each topic to find which topic it is most likely"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordToTopic(df):\n",
    "    word_to_topic = {}\n",
    "    num_of_words = {}\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        topic = row[1][\"topic_area\"]\n",
    "        content = re.sub(\"[^a-zA-Z\\&]+\", \" \", row[1]['content']).lower().split(\" \")\n",
    "        content = [ _ for _ in content if _ not in stopwords ]\n",
    "\n",
    "        if word_to_topic.get(topic, None) == None:\n",
    "            word_to_topic[topic] = {}\n",
    "\n",
    "        if num_of_words.get(topic, None) == None:\n",
    "            num_of_words[topic] = 0\n",
    "\n",
    "        for word in content:\n",
    "            if word_to_topic[topic].get(word, None) == None:\n",
    "                word_to_topic[topic][word] = 0\n",
    "            word_to_topic[topic][word] += 1\n",
    "            num_of_words[topic] += 1\n",
    "    \n",
    "    # for topic, value in word_to_topic.items():\n",
    "    #     for word, num_occurence in value.items():\n",
    "    #         value[word] = num_occurence / num_of_words[topic] * 100\n",
    "    \n",
    "    return (word_to_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainTest = pd.read_csv(\"sample.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 29047 entries, 0 to 29046\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   title         29047 non-null  object\n 1   url           29047 non-null  object\n 2   crawled_time  29047 non-null  object\n 3   date          29047 non-null  object\n 4   domain        29047 non-null  object\n 5   author        19635 non-null  object\n 6   content       29047 non-null  object\n 7   topic_area    29047 non-null  object\ndtypes: object(8)\nmemory usage: 2.0+ MB\n"
    }
   ],
   "source": [
    "trainTest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trainTest[: int(len(trainTest) * 0.9) ]\n",
    "test = trainTest[ int(len(trainTest) * 0.9): ]\n",
    "\n",
    "## call the function to generate it again or used the pre generated JSON file\n",
    "# word_to_topic = createWordToTopic(train)\n",
    "# with open(\"word_to_topic_raw_count.json\", \"w+\") as fp:\n",
    "#     json.dump(word_to_topic, fp, indent=2)\n",
    "word_to_topic = json.load(open(\"word_to_topic_raw_count.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dict_keys(['business', 'general', 'science', 'finance', 'tech', 'healthcare', 'automotive', 'environment', 'ai'])\n"
    }
   ],
   "source": [
    "print (word_to_topic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "123189\n"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for key, val in word_to_topic.items():\n",
    "    for word in val:\n",
    "        vocab.add(word)\n",
    "\n",
    "print (len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "manual_result = []\n",
    "for row in test[:2].iterrows():\n",
    "    actual_topic = row[1][\"topic_area\"]\n",
    "    \n",
    "    tokens = re.sub(\"[^a-zA-Z\\&]+\", \" \",row[1][\"content\"]).lower().split(\" \")\n",
    "    tokens = [ x for x in tokens if x != \"\" and x not in stopwords ]\n",
    "\n",
    "    highest_score = 0\n",
    "    guess_topic = \"\"\n",
    "    for topic, array in word_to_topic.items():\n",
    "        score = 0\n",
    "        for word in tokens:\n",
    "            score += math.log( (array.get(word, 0) + 1) / len(train[train[\"topic_area\"].str.contains(topic)]) + len(vocab) )\n",
    "        \n",
    "        if score > highest_score:\n",
    "            highest_score = score\n",
    "            guess_topic = topic\n",
    "    \n",
    "    if topic == actual_topic:\n",
    "        num_correct += 1\n",
    "    manual_result.append( {\"actual\": actual_topic, \"guess\": guess_topic} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "actual: science\nguess: environment\n\nactual: science\nguess: environment\n\n"
    }
   ],
   "source": [
    "for _ in manual_result:\n",
    "    for k, v in _.items():\n",
    "        print (f\"{k}: {v}\")\n",
    "    print ()"
   ]
  },
  {
   "source": [
    "# Naive Bayes Classifier (library)\n",
    "\n",
    "https://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainTest = pd.read_csv(\"train_test.csv\")\n",
    "\n",
    "## convert label to a number\n",
    "topic_areas = list(trainTest[\"topic_area\"].unique())\n",
    "topic_to_number = { topic_areas[idx]:idx for idx in range(len(topic_areas)) }\n",
    "\n",
    "trainTest[\"label\"] = trainTest[\"topic_area\"].apply(lambda x: topic_to_number[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    184128\n1     51634\n3     19115\n4      3855\n2      1768\n5       359\n7       335\n6       222\n8        15\nName: label, dtype: int64"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "trainTest[\"label\"].value_counts()\n",
    "## this might be bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainTest[\"content\"], trainTest[\"label\"], random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(strip_accents=\"ascii\", token_pattern=u\"(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b\", lowercase=True, stop_words=\"english\")\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_cv, y_train)\n",
    "predictions = naive_bayes.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score:  0.591955078184767\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}